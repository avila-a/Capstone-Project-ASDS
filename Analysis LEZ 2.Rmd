---
title: "Analysis LEZ 2"
author: '41783'
date: "25/02/2020"
output: 
  html_notebook:
    code_folding: show
    # keep_md: true
    df_print: paged
---

Basic configuration and loading of packages:

```{r setup, include=FALSE}
# rm(list = ls())
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.align = "center",
  out.width = '80%'
)
```

```{r message=FALSE, warning=FALSE}
# library(jsonlite)
library(tidyverse)
library(rvest)

library(sf) # Geo analysis
# library("rgdal")

library(DBI) # SQL db

library(panelView)
library(lubridate)
```


## Index of contents

* Read data from pre-processing
  + Nuts regions geographies
  + data from capstone_db
    * nuts_time (outcomes, controls and mediators)
    * treatment from Umweltzonen
* Do explorative analysis on the volatility of outcomes
* Create variables of interest
* Do analysis




## Read data from pre-processing:

### Nuts Regions geographies 

```{r}
# Characteristics of nuts3:

nuts.sf <- readRDS("data/nuts_sf_treated")

nuts.sf <- nuts.sf %>%
  dplyr::rename(
    geo = NUTS_ID,
    nuts_level = LEVL_CODE,
    country = CNTR_CODE,
    nuts_name = NUTS_NAME
  )

nuts.sf %>% as.data.frame() %>% head()
```

### Get data from SQLdb:

```{r}
db <- dbConnect(RSQLite::SQLite(), "data/capstonedb")
dbListTables(db)
```

Tables:

* airq_plans: List of (new) air quality plans
* nuts3: NUTS code, name and attainment status according to UAR and GZ, and minimum year for GZ
* umweltzonen: Treatment status of all zones in germany for each status with timing.
* nuts_time statistical data on NUTS regions (outcomes and controls)

#### Get nuts_time (outcomes, mediators and controls)

```{r}
nuts_time <- dbGetQuery(db, "SELECT * from nuts_time")
nuts_time %>% filter(time == 2006)
nuts_time %>% names()

nuts_time %>%
  filter(geo == "PL91") %>%
  select(time)
```

Create variables of interest (outcomes and controls)
(and possibly a restricted dataset only with those)

```{r}
## (for NUTS2)

### outcomes:

# hours worked per person (abseentism/temporality)
# (waiting to get data on hours)

nuts_time <- nuts_time %>%
  # dplyr::group_by(geo) %>% # 1000's hours/1000's employees
  mutate(hours_per_worker = hours_TOTAL/empl_TOTAL,
         hours_per_day = hours_per_worker/259) #approx

# Looks like correct
# dta %>%
#   ggplot() +
#   geom_histogram(aes(x = hours_per_day))

# wages per worker
# (waiting to get data on wages)

nuts_time <- nuts_time %>%
  # dplyr::group_by(geo) %>%
  mutate(wages_per_worker = 
           (wages_TOTAL*1000000)/(empl_TOTAL*1000), 
         wage_per_day = wages_per_worker/259) #approx

# # Looks like correct
# dta %>%
#   ggplot() +
#   geom_density(aes(x = wage_per_day))
# # +  facet_wrap(facets = vars(time))

# wage/hour worked (productivity?)

nuts_time <- nuts_time %>%
  # dplyr::group_by(geo) %>%
  mutate(wage_per_hour = 
           (wages_TOTAL*1000000)/(hours_TOTAL*1000)) #approx

# # Looks like correct
# dta %>%
#   ggplot() +
#   geom_density(aes(x = wage_per_hour))
# # +  facet_wrap(facets = vars(time))

# Unemployment
# (done)

# Employment rate (with respect to active pop or "wotking age pop")

nuts_time <- nuts_time %>%
  # dplyr::group_by(geo) %>%
  mutate(empl_rate_active = # both in thousands
           (empl_TOTAL*1000)/(active_T_Y_GE15*1000),
         empl_rate_wpop = # both in thousands
           (empl_TOTAL*1000)/pop_15_64)

## Some doubts left...
# dta$empl_rate_active %>% hist(breaks = 30) # Why above 1?
# dta$empl_rate_wpop %>% hist(breaks = 30)

 ### Controls: 

# Investment/capita (nuts 2)

nuts_time <- nuts_time %>%
  # dplyr::group_by(geo) %>%
  mutate(inv_per_worker = # both in thousands
           (inv_TOTAL*1000)/(empl_TOTAL*1000))

# investment per ¿employee? ( in Euros per head)
nuts_time <- nuts_time %>%
  mutate(inv_per_worker = # Inv in millions, empl in tousands 
           (inv_TOTAL*1000000)/(empl_TOTAL*1000))

nuts_time <- nuts_time %>%
  mutate(activity_rate = # Both in thousands
           (pop_total*1000)/(active_T_Y_GE15*1000))


# Proportion of sector's employment ( in which sectors has employment increased more, could this be linked to the application of a LEZ?)

nuts_time <- nuts_time %>%
  mutate(empl_rate_agriculture = empl_A/empl_TOTAL,
         empl_rate_industry = `empl_B-E`/empl_TOTAL,
         empl_rate_construction = `empl_F`/empl_TOTAL, 
         empl_rate_local_trade = `empl_G-I`/empl_TOTAL,# Wholesale and retail trade, transport, accommodation and food service activities
         empl_rate_financial = `empl_K`/empl_TOTAL,
         empl_rate_realstate = `empl_L`/empl_TOTAL,
         empl_rate_professional = `empl_M_N`/empl_TOTAL,
         empl_rate_public = `empl_O-Q`/empl_TOTAL,
         empl_rate_entretainment = `empl_R-U`/empl_TOTAL)

nuts_time <- nuts_time %>%
  mutate(gva_rate_agriculture = gva_A/gva_tot, # Agriculture, forestry and fishing
         gva_rate_industry = `gva_B_E`/gva_tot, # Industry (except construction)
         gva_rate_construction = `gva_F`/gva_tot, # Construction
         gva_rate_local_trade = `gva_G_J`/gva_tot,#  Wholesale and retail trade, transport, accommodation and food service activities
         gva_rate_work = `gva_K_N`/gva_tot, # Financial and insurance activities; real estate activities; professional, scientific and technical activities; administrative and support service activities
         gva_rate_public = `gva_O_U`/gva_tot) # Public administration and defence; compulsory social security; education; human health and social work activities; arts, entertainment and recreation; other service activities; activities of household and extra-territorial organizations and bodies

nuts_time %>%
  filter(substr(geo, 1, 2) == "DE") %>%
  select(geo, time, `gva_rate_work`, gva_tot, gva_rate_public)

# Educational level/capita (nuts 2)
# Dependency ratio (NUTS 2)

# Share of Women in the labor market
nuts_time <- nuts_time %>%
  # dplyr::group_by(geo) %>%
  mutate(active_F_share = # both in thousands
           `active_F_Y15-64`/`active_M_Y15-64`)

nuts_time$active_F_share %>% hist(breaks = 30)

## (for NUTS 3)

### Outcomes:

# Proportion of working age population

# Dependency rate

### Controls
# mean age
# mortality and birth rate

nuts_time %>% names()
 # others to do GDP and Productivity: GDP/capita, Investment/capita, wage levels, 
```


### Small analysis of the volatility of data:

```{r}
get_volatility <- function(data = nuts_time, 
                           CNTR_CODE = "DE", 
                           time_vector = 2000:2019,
                           variable = "gdp_eur_hab",
                           cluster_differences = "nuts1" # can be "country"
){
  temp <- data %>%
    arrange(geo, time) %>%
    dplyr::mutate(nuts1 = substr(geo, 3, 4),
           country = substr(geo, 1, 2)) %>%
    filter(
      country == CNTR_CODE,
      # !is.element(time, c(2008, 2009, 2010, 2011)),
    ) %>%
    dplyr::group_by(geo) %>%
    dplyr::mutate(perc_change_unit = (eval(parse(text=variable))-lag(eval(parse(text=variable))))/lag(eval(parse(text=variable)))*100) %>%
    ungroup() %>%
    dplyr::group_by("nuts1", time) %>%
    dplyr::mutate(mean_change_group = mean(perc_change_unit, na.rm = TRUE),
           perc_change_dev = perc_change_unit - mean_change_group) %>%
    ungroup() %>%
    select(geo, time, variable, mean_change_group, perc_change_unit, perc_change_dev)
  print(temp %>% summary)
  return(temp)
}
```


```{r}
temp2 <- get_volatility(data = nuts_time, CNTR_CODE = "DE",
                       time_vector = 2000:2019,
                       variable = "unempR_T")
# temp2

# pop_total, empl_EMP_TOTAL, inv_TOTAL, wages_TOTAL, mean_pol, 
```


### Nuts Regions geographies and treatment status

All nuts 3 and nuts 2 regions that had multiple LEZ were excluded from this data, so will have NA when I merge it with "dta".

```{r}
nuts_treated <- readRDS("data/nuts_sf_treated")


nuts_treated <- nuts_treated %>%
  dplyr::rename(
    geo = NUTS_ID,
    nuts_level = LEVL_CODE,
    country = CNTR_CODE,
    nuts_name = NUTS_NAME
  ) 

nuts_treated %>% as.data.frame() %>% head()
```

## Unite variables and treatment status:

```{r}
main_dta <- nuts_treated %>% 
  as.data.frame() %>%
  select(-geometry) %>%
  right_join(nuts_time, by = "geo")

main_dta %>% names()
main_dta %>% head()


# saveRDS(main_dta, "data/main_dta.rds")
```

### Searcher for NA values:

```{r fig.height= 8, fig.width= 6}
countries <- main_dta$country %>% unique()
countries <- countries[-c(3, 23, 34)]

# i <- 1
for (c in countries[1:length(countries)]){
  # print(i)
  # i <- i+1
  temp <- main_dta %>%
    filter(country == c)
  
  sum_na <- temp %>%
  dplyr::select(unempR_T, urb_index, gdp_eur_hab) %>%
  is.na() %>% colSums() %>% max()
  if(sum_na == nrow(temp)){
    print(paste0("The country ", c, " has no obs"))
    next
  }
  
  panelView(1 ~ unempR_T + urb_index + gdp_eur_hab, 
          data = temp, 
          index = c("geo","time"), 
          ignore.treat = TRUE,
          xlab = "Year", 
          ylab = "NUTS", 
          by.timing = TRUE,
          main = c
          # axis.lab.gap = c(0,60)
          # show.id = c(1:25), # number id of ones to show
          )
}
```


# Synthetic control method:

Manual: https://cran.r-project.org/web/packages/Synth/Synth.pdf

<!-- ```{r} -->
<!-- # install.packages("Synth") -->
<!-- library(Synth) -->
<!-- data(basque) -->
<!-- basque -->

<!-- dataprep.out <- -->
<!--   dataprep( -->
<!--     foo = basque -->
<!--     # main predictors (cavariates) -->
<!--     ,predictors= c("school.illit", -->
<!--                    "school.prim", -->
<!--                    "school.med", -->
<!--                    "school.high", -->
<!--                    "school.post.high" -->
<!--                    ,"invest" -->
<!--     ) -->
<!--     ,predictors.op = c("mean") -->
<!--     ,dependent = c("gdpcap") -->
<!--     ,unit.variable = c("regionno") -->
<!--     ,time.variable = c("year") -->
<!--     # Those that are used as time variant predictors -->
<!--     ,special.predictors = list( -->
<!--       list("gdpcap",1960:1969,c("mean")), -->
<!--       list("sec.agriculture",seq(1961,1969,2),c("mean")), -->
<!--       list("sec.energy",seq(1961,1969,2),c("mean")), -->
<!--       list("sec.industry",seq(1961,1969,2),c("mean")), -->
<!--       list("sec.construction",seq(1961,1969,2),c("mean")), -->
<!--       list("sec.services.venta",seq(1961,1969,2),c("mean")), -->
<!--       list("sec.services.nonventa",seq(1961,1969,2),c("mean")), -->
<!--       list("popdens",1969,c("mean"))) -->
<!--     ,treatment.identifier = 17 -->
<!--     ,controls.identifier = c(2:16,18) -->
<!--     ,time.predictors.prior = c(1964:1969) -->
<!--     ,time.optimize.ssr = c(1960:1969) -->
<!--     ,unit.names.variable = c("regionname") -->
<!--     ,time.plot = c(1955:1997) -->
<!--   ) -->


<!-- synth.out <- synth(dataprep.out) -->

<!-- data_synth <- main_dta %>% as.data.frame() %>% -->
<!--   # Controls are 50km away from cities that have applied a LEZ o similar mesure, probably have to bi limited to one treated only  -->
<!--   filter(treated_uz | contr_restr_50km) %>%  -->
<!--   # Not necessary, If I use a variable that has only NUTS 2 regions -->
<!--   filter(nuts_level == 2)  %>%  -->
<!--   # only keep one treated and all controls -->
<!--   filter(contr_restr_50km | geo == "DE30") -->
<!-- dataprep(foo = nuts_try, -->
<!--                 predictors = values, # This is wrong -->
<!--                 dependent = values, -->
<!--                 unit.variable = NUTS_ID, -->
<!--                 time.variable = time, -->
<!--                 treatment.identifier = "HR043", -->
<!--                 ) -->
<!-- nuts_try -->
<!-- ``` -->

More info:

http://www.mit.edu/~jhainm/fqa.htm

http://yiqingxu.org/teaching/17802/synth.pdf

https://www.stata.com/meeting/uk17/slides/uk17_Cerulli.pdf

https://blogs.worldbank.org/impactevaluations/evaluating-regulatory-reforms-using-the-synthetic-control-method
https://jech.bmj.com/content/72/8/673

Possibly then complement by 
### **Fast and reliable computation of generalized synthetic controls (MSCMT package)**
(look at Mendelay paper and to https://cran.r-project.org/web/packages/MSCMT/index.html)

## MASC

> See "Implementation ECI borrador


# Generalized Synthetic control method (experiment)

  * Unrestricted weights (can be negative or positive)
  * Calculation of ATE
  * Calculation of Matrix completion methods.

1. Try with NUTS 2 regions. 
  - Variable: Unemployment rate total, case: Berlin
2. Try with NUTS 3 regions. 
  - Variable: Employed population rate, case: Aachen
  


```{r}
library(gsynth)
library(lubridate)

# Transformate treatment var into a dummy variable
# (Would be good to do this before.)

main_dta <- main_dta %>%
  mutate(
    is_inagurated = case_when(
      is.na(year_stage1) ~ 0, # If its control, never inagurated
      time < year_stage1 ~ 0, # If its before the inaguration
      TRUE ~ 1 # if neither, then its after
    ),
    # create a placebo treatment
    is_inagurated_3y = case_when(
      is.na(year_stage1) ~ 0, # If its control, never inagurated
      time < year_stage1 - 3 ~ 0, # If its more than 3 years beforethe inaguration
      TRUE ~ 1 # if neither, then its after
    ),
    is_announced = case_when(
      is.na(year_stage1) ~ as.integer(0), # If its control, never
      is.na(year_announced) ~ NA_integer_, # If i dont have date of announcment, forget observaton
      time < year_announced ~ as.integer(0), # If its before
      TRUE ~ as.integer(1) # if neither, then its after
    ),
    is_announced_2y = case_when(
      is.na(year_stage1) ~ as.integer(0), # If its control, never
      is.na(year_announced) ~ NA_integer_, # If i dont have date of announcment, forget observaton
      time < year_announced - 2 ~ as.integer(0), # If its before
      TRUE ~ as.integer(1) # if neither, then its after
    )
  )

main_dta %>%
  select(geo, time, LEZ,
         Stage_1, is_inagurated,
         Announcment, is_announced,
         )  %>%
  filter(!is.na(Stage_1))

# Prepare data for NUTS 2 synthetic control -> restrict controls
n2_dta <- main_dta %>%
  # FILTER SPAIN AND GREECE! (for those applied close to the finantial crisis)
  filter(!(country %in% c("ES", "EL", "PT", "PL", "TR"))) %>%
# https://data.worldbank.org/indicator/SL.UEM.TOTL.NE.ZS?locations=DE-ES-GR-PL-NL-PT-LU-CZ-HU
  filter(treated_uz | contr_restr_60km) %>% # Controls are 50km away from cities that have applied a LEZ o similar mesure, probably have to bi limited to one treated only 
  filter(nuts_level == 2) # Not necessary, If I use a variable that has only NUTS 2 regions

# panelView(Y ~ D, data = main_dta,  index = c("id","time"), pre.post = TRUE) 

# {
#   n2_dta <- n2_dta[complete.cases(
#   n2_dta %>%
#     select(geo, time, # unempR_T
#            # mean_pol, # outcome and mediator
#            # gdp_eur_hab # urb_index, active_F_share, gdp_eur_hab, wages_per_worker # controls
#            )), ]
# 
# incomplete <- which((n2_dta$geo %>% table(useNA = "no")) < length(unique(n2_dta$time)))
# 
# n2_dta <- n2_dta %>%
#   filter(!(geo %in% names(incomplete))) %>%  # Delete incomplete series
#   mutate(num_id = as.numeric(as.factor(geo))) %>%
#   arrange(num_id)
# }



# incomplete_maps <- nuts.sf.treated %>%
#   filter(NUTS_ID %in% names(incomplete))
# 
# leaflet() %>% addTiles() %>%
#   addPolygons(data = incomplete_maps)

```

### Try Gsynth N2

```{r}

# data_one_treated <- n2_dta %>%
#   filter(geo == "DE30" | contr_restr_60km)

n2_dta <- n2_dta %>%
  filter(geo != "DEG0")

out_n2 <- gsynth(unempR_T ~ is_announced_2y,
                 + wages_per_worker
                 + gdp_eur_hab,
                 urb_index,
               data = n2_dta, 
               index = c("geo","time"), 
               na.rm = TRUE, # remove 
               force = "two-way", # unit or time fixed effects or both?
               # CV = TRUE, 
               r = c(0, 3), # an integer specifying the number of factors. If CV = TRUE, the cross validation procedure will select the optimal number of factors from r to 5.
               se = TRUE, 
               EM = TRUE, # Gobillon and Magnac (takes a lot of time)
               inference = "parametric",
               # estimator = "mc",
               nboots = 100,  #500
               parallel = TRUE, cores = 5, # if do parallel and how many cores
               min.T0 = 5) # Has to be larger than r.max+1

out_n2 %>% print()
plot(out_n2)#, ylim = c(-5000, 2000))

# plot all data:
plot(out_n2, type = "counterfactual", 
     raw = "all", main="") # Why the final dip?

plot(out_n2, type = "counterfactual", 
     raw = "none", main="") # Why the final dip?

for (i in n2_dta[n2_dta$treated_uz == TRUE, ]$geo %>% unique()){
  plot(out_n2, type = "counterfactual", id = i) # Berlin
}
# 
# plot(out_n2, type = "counterfactual", id = "DE21") # Munich
# plot(out_n2, type = "counterfactual", id = "DE13") # Freiburg
# plot(out_n2, type = "counterfactual", id = "DE50") # Bremen 
# plot(out_n2, type = "counterfactual", id = "DE92") # Hannover 
# plot(out_n2, type = "counterfactual", id = "DE60") # Bremen 

# plot(out_n2, type = "raw", theme.bw = TRUE)

# We can also add two 5 to 95% quantile bands of the treated and control outcomes as references to make sure the estimated counterfactuals are not results of severe extrapolations.

# NOTE: THIS IS STRAGE, WHY ONLY BERLIN? I think because its the only treated
plot(out_n2, type = "counterfactual", 
     raw = "band", xlab = "Time", 
     # ylim = c(-5,35),
     theme.bw = TRUE)


# plot(out_n2, type = "loadings")
```

```{r}
# out_n2$xi
```


Things left to do:
 * Know the donnor pool of each treated
 * know the average characteristics of each treated and its synthetic control + the variance (max and min of those characteristics from the donor pool)
 * Unemployment counterfactual for ATE goes bellow 0%,

```{r}
# Know the donor pool of each treated:

weights_result <- out_n2$wgt.implied %>% 
  as_tibble(rownames = "NUTS_ID")


donor_pool <- nuts.sf.treated %>%
  filter(LEVL_CODE == 2) %>%
  left_join(weights_result) %>%
  filter(!is.na(DE30))

donor_pool %>% as.data.frame() %>%
  select(NUTS_ID, DE30, DE13)


leaflet() %>% addTiles() %>%
  addPolygons(data = donor_pool, weight = 0.5, color = "black",
              fillColor = ~colorQuantile("RdYlGn",
                      donor_pool$DE30)(donor_pool$DE30),
              popup = paste0(
                "<b>", donor_pool$NUTS_ID, "</b> <br/>",
                "<b> Weight Berlin </b>", donor_pool$DE30, "<br/>",
                "<b> Weight Freiburg </b>", donor_pool$DE13, "<br/>")
              )
```


### Try Gsynth N3

```{r}
# Prepare data for NUTS 2 synthetic control -> restrict controls
n3_dta <- main_dta %>%
  filter(treated_uz | contr_restr_50km) %>% # Controls are 50km away from cities that have applied a LEZ o similar mesure, probably have to bi limited to one treated only 
  filter(nuts_level == 3)

# REVIEW DATA
n3_dta %>%
  select(geo, time, LEZ,
         Stage_1, is_inagurated,
         Announcment, is_announced,
         )  %>%
  filter(!is.na(Stage_1))

out_n3 <- gsynth(empl_rate_wpop ~ is_inagurated, 
               data = n3_dta, 
               index = c("geo","time"), 
               na.rm = TRUE, # remove 
               force = "two-way", # unit or time fixed effects or both?
               # CV = TRUE, 
               # r = c(0, 4), # an integer specifying the number of factors. If CV = TRUE, the cross validation procedure will select the optimal number of factors from r to 5.
               se = TRUE, 
               # EM = TRUE, # Gobillon and Magnac (takes a lot of time)
               # inference = "parametric", 
               nboots = 100,  #500
               parallel = TRUE, cores = 5, # if do parallel and how many cores
               min.T0 = 7) # Has to be larger than r.max+1

n3_dta$is_inagurated %>% table()

out_n3 %>% print()
plot(out_n3)#, ylim = c(-5000, 2000))

# plot all data:
plot(out_n3, type = "counterfactual", 
     raw = "all", main="") # Why the final dip?

plot(out_n3, type = "counterfactual", 
     raw = "none", main="") # Why the final dip?

plot(out_n3, type = "counterfactual", id = "DE300") # Berlin

# plot(out_n2, type = "raw", theme.bw = TRUE)

# We can also add two 5 to 95% quantile bands of the treated and control outcomes as references to make sure the estimated counterfactuals are not results of severe extrapolations.

# NOTE: THIS IS STRAGE, WHY ONLY BERLIN? I think because its the only treated
plot(out_n3, type = "counterfactual", 
     raw = "band", xlab = "Time", 
     # ylim = c(-5,35),
     theme.bw = TRUE)

plot(out_n3, type = "loadings")

dim(out_n3$wgt.implied)
## [1] 38  9
sort(out_n3$wgt.implied[,8])
```

Notes on results:

  * A lot of removed treated units
  * Factor loadings do not coincide, maybe some restriction of controls would be good.


```{r}
data(gsynth)

# ?gsynth
turnout
simdata

# out <- gsynth(Y ~ D + X1 + X2, data = simdata, 
#               index = c("id","time"), 
#               force = "two-way", CV = TRUE, 
#               r = c(0, 5), se = TRUE, 
#               inference = "parametric", nboots = 1000, 
#               parallel = FALSE)


```


```{r}
# install.packages('gsynth', type = 'source')
library(gsynth)

out2 <- gsynth(gdp_eur_hab ~ treat_gz + gva_A + gva_B_E + 
                 gva_F + gva_G_I + gva_J + gva_K + gva_L + 
                 gva_M_N + gva_O_U, 
               data = data_synth_c, 
               index = c("geo","time"), 
               force = "two-way", 
               CV = TRUE, 
               r = c(0, 4), # an integer specifying the number of factors. If CV = TRUE, the cross validation procedure will select the optimal number of factors from r to 5.
               se = TRUE, 
               # EM = TRUE, # Gobillon and Magnac (takes a lot of time)
               inference = "parametric", nboots = 100,  #500
               parallel = TRUE,
               min.T0 = 6) # Has to be larger than r.max+1

# Idea: include countries/NUTS 2 regions and others as FE to control for heterogeneity?

# NOTE: I need to correct from assigning treatment by a point to assygn treatment by the area. If not, some NUTS3 in Paris and London will be counted as controls! 
# They should eather be treated or cease to be controls [ ]
# !!!
```




```{r}
out_n2 %>% print()
out2$est.att
out2$est.avg
out2$est.beta
out2$Y.ct %>% as.data.frame() # pred counterfactuals for the treated units.
out2$Y.co %>% as.data.frame()
out2$eff %>% as.data.frame()  # Y - predicted Y(0);
out2$eff.cnt %>% as.data.frame() # Y - predicted Y(0); rearranged based on the timing of the treatment.

# Inspect results, how are the estimated tratement effects for all treated?

out2$eff.cnt %>% as.data.frame() %>% # Y - predicted Y(0);
  rownames_to_column(var = "year") %>% 
  mutate(year = as.numeric(year)) %>%
  pivot_longer(-year, names_to = "geo", 
               values_to = "y_minus_y0") %>%
  left_join(data) -> all_treat_effects

all_treat_effects %>%
  ggplot(aes(x = year, y = y_minus_y0)) +
  geom_line( aes(colour = nuts_name))
# Note: I need to see if this continues to happen for random pacebo treatments 
all_treat_effects
```

_Questions:_ 

* What is the CV procedure of gsynth?
* What is the bootstraping procedure of gsynth?

```{r}
# out2$est.co$residuals %>% as.data.frame()
# plot(out2, ylim = c(-0.4, 0.02))
plot(out_n2)#, ylim = c(-5000, 2000))

plot(out_n2, type = "counterfactual", 
     raw = "all", main="") # Why the final dip?

plot(out2, type = "counterfactual", 
     raw = "none", main="") # Why the final dip?

plot(out_n2, type = "counterfactual", id = "DE30") # Berlin
# plot(out2, type = "counterfactual", id = "FR101") # Paris
# plot(out2, type = "counterfactual", id = "HU110") # Budapest
plot(out2, type = "counterfactual", id = "DE111") # Stuttgart
plot(out2, type = "counterfactual", id = "DE212") #	München
# plot(out2, type = "counterfactual", id = "UKI32") #	Westminster (LDN)
# LONDON BOROUGHS! UKI31 - UKI45
# UKI31	Camden and City of London	
# UKI32	Westminster
# UKI33	Kensington & Chelsea and Hammersmith & Fulham		
# UKI34	Wandsworth		
# UKI41	Hackney and Newham		
# UKI42	Tower Hamlets
# UKI43	Haringey and Islington		
# UKI44	Lewisham and Southwark		
# UKI45	Lambeth
# UKI51	Bexley and Greenwich		
# UKI52	Barking & Dagenham and Havering		
# UKI53	Redbridge and Waltham Forest		
# UKI54	Enfield

data_synth_c %>%
  filter(substr(geo, 1, 2) == "UK") %>%
  arrange(geo) %>%
  filter(geo == "UKI32")

# London (LEZ) initial date (2008) is WRONG, it is the ULEZ one (2019)
# The CCS was introduced in 2003, expansion on 2007
```


## Extras:

### Try to extract kmls (geographies of all LEZ):
_NOTE_: The files are encrypted, I need to deencrip them before. C#.
```{r}
# library(rgdal)
# belg_ghent = readOGR(dsn = "~/A - Estudios/LSE - Applied Social Data Science/Project/PM2.5/Data Europe/BelgienGent.kmls")
# ??readOGR
```

### Good info on spatial analysis:
https://mgimond.github.io/Spatial/uncertainty-in-census-data.html

### Fast and reliable computation of generalized synthetic controls (MSCMT package)
(look at Mendelay paper and to https://cran.r-project.org/web/packages/MSCMT/index.html)

### New methodologies (2019) on multiple timing and periods DiD:
https://andrewcbaker.netlify.com/2019/09/25/difference-in-differences-methodology/

### Abadie and L'Hour code for SCM with penalization for interpolation biases:

https://github.com/jlhourENSAE/regsynth

### RNN - based methods: Predict the synthetic control with NN:
https://ideas.repec.org/p/arx/papers/1712.03553.html

### "CausalInference" the Google made package to estimate Bayesian Structural Time Series Models:
https://google.github.io/CausalImpact/

### Critiques to synth methods with CV if done improperly:
- Look at Mendeley biblography on 04/2020

### Non-parametric Synth (Italian researcher - on STATA)
https://ideas.repec.org/c/boc/bocode/s458398.html
https://www.sciencedirect.com/science/article/pii/S0165176519301788?entityID=https%3A%2F%2Flse.ac.uk%2Fidp&pes=vor 

## Similar analysis with code and other methodologies: (to read and review)
https://andrewcbaker.netlify.com/2020/01/01/what-can-we-say-about-medical-marijuana-and-opioid-overdose-mortality/

## How much pollution comes from cars? (and other info from the EC):
https://ec.europa.eu/environment/air/cleaner_air/

## SUMMARY ST

How many LEZ in Europe?

```{r}
uar_dta <- read.csv(file = "data/UAR_data/uar.csv")

uar_dta$lez %>% sum(na.rm = TRUE)
```


## PLOTS:

### Small analysis of the application of LEZ zones in Germany:

```{r}
umweltzonen <- readRDS("data/umweltbundesamt/umweltzonen.rds")
# dbGetQuery(db, "SELECT * from umweltzonen")

um <- umweltzonen %>%
  mutate(Stage_1 = as.Date(Stage_1, "%d.%m.%Y"),
         Stage_2 = as.Date(Stage_2, "%d.%m.%Y"),
         Stage_3 = as.Date(Stage_3, "%d.%m.%Y"),
         Announcment = as.Date(Announcment)) %>%
  mutate(Announcment = if_else(is.na(Announcment), 
                               Stage_1, Announcment)) %>%
  arrange(Stage_3) %>%
  arrange(Stage_2) %>%
  arrange(Stage_1) %>%
  arrange(Announcment) %>%
  mutate(num_id = 1:n())

all <- expand.grid(LEZ = um$LEZ, 
                   year = seq(2006, 2018),
                   month = seq(01, 12), 
                   # month = c("01", "02", "03", "04", "05", "06",
                   #          "07", "08", "09", "10", "11", "12"), 
                   stringsAsFactors = FALSE)

panel_zones <- all %>%
  left_join(um) %>%
  mutate(date = as.POSIXct(paste0(year,"-", month, "-01" )),
         # LEZ = substr(LEZ, 1, 24),
         LEZ = sub(" *\\(.*", "", LEZ), # Everything until a "("
         # If i don't have the date of announcemnt I do as if there were none
         LEZ = sub("und Umgebung", "", LEZ), # Everything until a "("
         # If i don't have the date of announcemnt I do as if there were none
         Announcment = if_else(is.na(Announcment), Stage_1, Announcment),
         
         status = case_when(
           date <= Announcment ~ 0, # not applied
           date <= Stage_1 ~ 1, # announced
           date <= Stage_2 ~ 2, # stage 1
           # If they don't have a stage 2 then they stayed in 1
           is.na(Stage_2) ~ 2,
           date <= Stage_3 ~ 3, #stage 2
           is.na(Stage_3) ~ 3,
           date > Stage_3 ~ 4, #stage 3
           # is.na(Announcment) ~ 0,
           # date < Stage_1
           TRUE ~ 0
         )
         # status = if_else(date < Announcment, 0, missing = 0,
         #                 if_else(date < Stage_1, 1,
         #                         if_else(date < Stage_2, 2, 
         #                                 if_else(date < Stage_3, 3, 4, 
         #                                         missing = 3))))
         # date_factor = paste0(year(date), "-", ifelse(
         #   month(date)<9, paste0("0", month(date)), month(date))),
         ) %>%
  arrange(Stage_3) %>%
  arrange(Stage_1) %>%
  arrange(Announcment) %>%
  mutate(LEZ_number = paste0(LEZ, " - ", num_id)) %>%
  mutate(LEZ_factor = fct_inorder(LEZ_number)) %>%
  select(LEZ_number, date, status, year, month, LEZ_factor)

panel_zones %>% head()
```



```{r, fig.height=8, fig.width= 7.5}

pv_timeline <- panelView(month ~ status, data = panel_zones, 
          index = c("LEZ_factor", "date"), 
          by.timing = TRUE, 
          axis.lab.gap = c(23,0), 
          # axis.lab = "time",
          background = "white",
          main = "",
          xlab = "", ylab = "",
          na.rm = FALSE,
          pre.post = TRUE,
          legend.labs = c("No LEZ applied", "LEZ announced", 
                          "Only red, yellow or green sticker",
                          "Only yellow or green sticker", 
                          "Only green stiker"),
          color = c("grey90", "grey60", "red1", 
                    "yellow1", "green4")) +
  theme(legend.box.margin = margin(l = -5, unit = "cm"))

# png(filename = "images/LEZ application germany_3.png",
#     width = 780, height = 1000, res = 100)

pv_timeline +
  # # Black vertical segments (+.5 because the graph starts at 1)
  # geom_segment(x = 33.5, xend = 33.5, y = 2.5, yend = 31.5, 
  #              size = 1.5) +
  geom_segment(x = 33.5+12, xend = 33.5+12, y = 31.5, yend = Inf, 
               size = 1.5) +
  geom_segment(x = 12*12, xend = 12*12, y = 0.5, yend = 2.5, 
               size = 1.5) +
  # geom_vline(xintercept = 33, size = 2) + 
  # geom_vline(xintercept = 12*10, size = 1.5)

  # White shading areas 
  # annotate("rect", xmin = 0, xmax=Inf, ymin=32.5, ymax=Inf,
  #                             alpha = 0.6, fill='white') +
  # annotate("rect", xmin = 0, xmax=Inf, ymin=0.5, ymax=4.5,
  #                             alpha = 0.6, fill='white') +
  # Black-shading year markers
  annotate("rect", xmin = 0.5, xmax=1.5, ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*1 + 0.5, xmax=12*1 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*2 + 0.5, xmax=12*2 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*3 + 0.5, xmax=12*3 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*4 + 0.5, xmax=12*4 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*5 + 0.5, xmax=12*5 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*6 + 0.5, xmax=12*6 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*7 + 0.5, xmax=12*7 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*8 + 0.5, xmax=12*8 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  annotate("rect", xmin = 12*9 + 0.5, xmax=12*9 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
    annotate("rect", xmin = 12*10 + 0.5, xmax=12*10 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
    annotate("rect", xmin = 12*11 + 0.5, xmax=12*11 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
    annotate("rect", xmin = 12*12 + 0.5, xmax=12*12 + 1.5, 
           ymin=0.5, ymax=Inf,
           alpha = 0.2,
           fill = "black", size = 1.5) +
  # red square
  annotate("rect", xmin = 1, xmax=Inf, ymin=2.5, ymax=31.5,
           alpha = 0.6, color = "red", 
           fill = NA, size = 1.5) +
  # Text
  geom_curve(aes(x = 8*12-4, y = 42, xend = 33.5+12, yend = 34), 
             colour = "black", 
             size=1, 
             curvature = -0.2,
             arrow = arrow(length = unit(0.03, "npc"))
             ) +
  geom_label(aes(x = 8*12-4, y = 42, 
                 label = "End of Scrappage\n Program"), 
             hjust = 0, 
             vjust = 0.5, 
             colour = "black", 
             fill = "white",
             # label.size = NA, 
             # family="Helvetica", 
             size = 5)

dev.off()

# png(filename = "images/LEZ application germany_clean.png",
#     width = 780, height = 1000, res = 100)
pv_timeline
# dev.off()

# devtools::install_github('xuyiqing/panelView')
# RColorBrewer::brewer.pal.info
pv_timeline
```

> NOTE! I NEED TO FIND THE DATE OF ANOUNCEMT OF THE MISSING LEZ!

(Image saved in png file)

Simple Version:


```{r, fig.height=8, fig.width= 9}
# png(filename = "LEZ application germany.png", 
#     width = 750, height = 1000, res = 100)
panelView(month ~ status, data = panel_zones, 
          index = c("LEZ_factor", "date"), 
          by.timing = TRUE, 
          axis.lab.gap = c(23,0), 
          # axis.lab = "time",
          background = "white",
          main = "",
          xlab = "", ylab = "", pre.post = TRUE,
          legend.labs = c("No LEZ applied", 
                          "Only red, yellow or green sticker", 
                          "Only yellow or green sticker", "Only green stiker"),
          color = c("grey90", "red1", "yellow1", "green4"))
# dev.off()

# devtools::install_github('xuyiqing/panelView')
# RColorBrewer::brewer.pal.info
```

(Image saved in png file)

### Map of cities

```{r}
# Get data on cities with LEZ
umweltzonen_map <- um %>%
  mutate(in_sample = 
           ifelse(year_announced < 2009 | is.na(year_announced), 
                            "Excluded", "Included"))

# umweltzonen
# 
# main_dta %>%
#   filter(LEZ == "Berlin") %>%
#   select(LEZ, gdp_mio_eur)


# Merge it with cities' NUTS characteristics
umweltzonen_map <-  main_dta %>%
  filter(treated_uz == TRUE) %>%
  select(LEZ, gdp_mio_eur) %>%
  group_by(LEZ) %>% 
  summarise_all(function(x){mean(x, na.rm =  TRUE)}) %>%
  right_join(umweltzonen_map) %>%
  st_as_sf()

# Get coodinates and characteristics of 3 "control" regions

nuts_time %>%
  filter(geo %in% c("DE402", "DED41", "DEF03")) %>%
  select(geo, gdp_mio_eur) %>%
  group_by(geo) %>%
  summarise_all(function(x){mean(x, na.rm =  TRUE)})

controls_df <- data.frame(LEZ = c("Lübeck", "Cottbus", "Chemnitz"),
                   in_sample = rep("Control", 3),
                   gdp_mio_eur = c(2865, 6969, 6671),
                   lat = c(53.85678, 51.760325, 50.835651),
                   lon = c(10.67413, 14.325153, 12.929591)
                   )

temp_sf <- controls_df %>%
  st_as_sf(coords = c("lon", "lat"),
           crs = "+proj=longlat +datum=WGS84 +no_defs")

umweltzonen_pro <- umweltzonen_map %>%
  select(LEZ, in_sample, gdp_mio_eur) %>%
  rbind(temp_sf)

umweltzonen_pro

separated_coord <- umweltzonen_map %>%
    mutate(lat = unlist(map(umweltzonen_map$geometry,1)),
           long = unlist(map(umweltzonen_map$geometry,2)))

separated_coord_control <- temp_sf %>%
  filter(in_sample == "Control") %>%
    mutate(lat = unlist(map(temp$geometry,1)),
           long = unlist(map(temp$geometry,2)))

# Get basemap for Germany

# library("rnaturalearth")
# library("rnaturalearthdata")
# 
# germany <- ne_countries(scale = "medium", 
#                       country = "germany",
#                       returnclass = "sf")

germany2 <- nuts_treated %>%
  filter(country == "DE") %>%
  st_combine() %>% 
  st_union()

nuts_treated_DE <- nuts_treated %>%
  filter(country == "DE") %>%
  filter(nuts_level == 3)
```


```{r, fig.height= 9, fig.width=7.5}
library(scales)

# Careful! This NUTS-SF is the one after all the "Umparse UAR php" R file is run and before some duplicate redions are excluded.
nuts_treatment_status_DE <- nuts.sf %>%
  st_join(umweltzonen) %>%
  filter(CNTR_CODE == "DE") %>%
  filter(LEVL_CODE == 3)

nuts_treatment_status_DE %>% plot(max.plot = 2)
```


```{r, fig.height= 9, fig.width=7.5}

pdf(file = "images/GermanNutsAndLEZ.pdf",
    width = 7.5, height = 10.5)

# ggplot() +
#   # geom_sf(data = germany2) +
#   geom_sf(data = nuts_treated_DE, 
#           aes(fill = "black"))

# temp <- read_sf("data/NUTS3/ref-nuts-2016-01m") %>%
#   select(-FID) %>%
#   filter(LEVL_CODE %in% c(2, 3)) %>%
#   st_make_valid()
# 
# mapview::mapView(temp %>% filter(LEVL_CODE == 3))


ggplot() +
  # geom_sf(data = germany2) +
  geom_sf(data = nuts_treatment_status_DE, 
          aes(fill = "black"),
          show.legend = FALSE) +
  geom_sf(data = nuts_treatment_status_DE %>% 
            filter(with_umweltzonen_60km == FALSE), 
          aes(fill = "")) +
  geom_sf(data = umweltzonen_pro, 
          aes(color = in_sample, size = gdp_mio_eur)) +
  # geom_sf_label(data = umweltzonen, 
  #         aes(label = num_id, size = 3)) +
  ggrepel::geom_label_repel(
    data = separated_coord,
    aes(x = lat,
        y = long,
        label = num_id),
    size = 3,
    min.segment.length = 0.4, segment.color = "white"
    # family = "Times New Roman"
    # arrow = arrow(length = unit(0.02, "npc"))
    # point.padding = NA
    # box.padding = unit(2, "lines")
  ) +
  ggrepel::geom_label_repel(
    data = controls_df,
    aes(x = lon,
        y = lat,
        label = LEZ),
    size = 3,
    min.segment.length = 0.4, segment.color = "white"
    # family = "Times New Roman"
    # arrow = arrow(length = unit(0.02, "npc"))
    # point.padding = NA
    # box.padding = unit(2, "lines")
  ) +
  # Asthetics
  labs(x = "", y = "",
       color = "In Sample:",
       size = "Regional GDP:",
       fill = "Control Region"
       ) +
  scale_size_area(max_size = 12, 
                  labels = scales::label_dollar(prefix = "€")) +
  # scale_size_continuous() +
  scale_fill_manual(values = c("grey70", "grey0"),
                    labels = c("Yes", "No"),) +
  guides(color = guide_legend(reverse = TRUE)) +
  theme_bw() +
  theme(
    legend.spacing = unit(0.4, "cm"),
    legend.position= "top",
    # legend.box.margin = margin(l = -1, unit = "cm"),
    legend.background = element_rect(color = "darkgray"),
    legend.box = "vertical"
  ) +
  update_geom_defaults("point", list(size = 9))
  

dev.off()

  # coord_sf(crs = "+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs ")
```


```{r}
p <- ggplot(mpg, aes(displ, hwy, size = hwy)) +
   geom_point()
p
p + scale_size("Highway mpg")
p + scale_size(range = c(0, 10))

# If you want zero value to have zero size, use scale_size_area:
p + scale_size_area()

# Binning can sometimes make it easier to match the scaled data to the legend
p + scale_size_binned()
```


```{r, fig.height= 10, fig.width=7}
tm_shape(germany) +
    tm_borders() +
tm_shape(umweltzonen) +
    tm_symbols(col = "in_sample",
               size = "gdp_mio_eur", 
               scale = 4) +
    tm_text("num_id", size = 0.7, 
            # auto.placement = TRUE
            ) +
  tm_layout(legend.bg.color = "white")
    
tm_legend(show = FALSE)

umweltzonen
```


