---
title: "Get data from GZ and UAR and unite the rest:"
author: "Antonio Avila"
date: "06/02/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE}
library(jsonlite)
library(tidyverse)
library(rvest)
library("rgdal") # ReadOGR
# setwd("~/A - Estudios/LSE - Applied Social Data Science/Project/PM2.5/Data Europe/")
```

## Get data from Urban Access Regulations (webpage)

1. Read the data:

```{r}
uar <- read_json(path = "Urb. Access Reg data mapapi.txt", 
                      simplifyVector = TRUE)[[1]]
# "Low Emission Zone in place since the 1st" (or similar, say the day of the beguinning.)
uar %>% head(2)
uar[4:7, 2]
# I could make a scraper that goes to every page and gets more information.

# From the "introtext" I colud also get some information about when it started:
uar$introtext[2] %>% cat()
```

Note: Look if I can scrape more info from the pages or descriptions.

Get the meanings of the colours:

https://urbanaccessregulations.eu/userhome/map#car

```{r}
uar <- uar %>%
  mutate(
    lez = ifelse(scheme_color == 1, TRUE, FALSE),
    urb_toll = ifelse(scheme_color == 2, TRUE, FALSE),
    other_reg = ifelse(scheme_color == 3, TRUE, FALSE),
    em_scheme = ifelse(scheme_color == 4, TRUE, FALSE),
    city_latitude = as.numeric(city_latitude),
    city_longitude = as.numeric(city_longitude),
    scheme_color = as.numeric(scheme_color)
  ) %>%
  mutate(scheme = case_when( 
         scheme_color == 1 ~ "lez",
         scheme_color == 2 ~ "urban toll",
         scheme_color == 3 ~ "other regulations",
         scheme_color == 4 ~ "pollution emergency") %>% as.factor())

uar %>% head()

uar %>% # All mesures are included
  filter(str_detect(uar$cityname, "Paris"))
# uar %>% dim() # Almost 700 legislations!
```


Read the scheme data

```{r}
# This gives me the lists of names and their Ids
allscheme <- read_json(path = "AllScheme.txt", 
                       simplifyVector = TRUE)[[1]]
write.csv(x = uar, file = "UAR_data/uar.csv")
write.csv(x = allscheme, file = "UAR_data/id_name_pairs.csv")
```


## Get the data grom GreenZones.eu

```{r}
library(DBI)
con <- dbConnect(RSQLite::SQLite(), 
                 dbname = "Greenzonesapp/db_greenzones.db")

# Explore:
dbListTables(con)
dbListFields(con, "Land")
DBI::dbGetQuery(con, "SELECT * from Zonenvorwarnung") 
# Kraftstoff -> Fuel
# SondereigenschaftAusprägung "specifics" look at "SondereigenschaftAusprägungTranslation".
# This works with "ZoneAusnahmeMap"(Zone exception map)

# This gives which distictives can enter where:
DBI::dbGetQuery(con, "SELECT * from ZonePlaketteMap") %>% head()
DBI::dbGetQuery(con, "SELECT * from Plakette") # Details and name of distinctives


# Vehicles:
# Types of vehicles and their category
DBI::dbGetQuery(con, "SELECT * from Fahrzeugart") %>% head()
```


```{r}
zones <- DBI::dbGetQuery(con, "SELECT Zone.*, Land.TitelLocal 
                         from Zone
                         LEFT JOIN Land ON Zone.LandId = Land.Id")

# Figure out the translation for datetimes:
# (PROBABLY ADD A COUPLE MORE TO INCREASE ACCURACY? WORKS PERFECT NOW...)
dates <- c("20080101", "20150901", "20190502", "20170101")
dates <- as.integer(lubridate::ymd(dates))
numbers <- c(633347424, 635766624, 636923520, 636188256)
lm_dates <- lm(dates ~ numbers, model = TRUE)
lm_dates
plot(dates, numbers, type="b")

zones %>% head()

zones <- zones %>%
  mutate(
    InPlaceSince = lubridate::as_date(
      floor(InkraftSeit*lm_dates$coefficients[2])/1000000000 + lm_dates$coefficients[1]),
    lat = (MaxLatitude + MinLatitude)/2,
    lon = (MaxLongitude + MinLongitude)/2
    ) %>%
  select(-c("InkraftSeit"))

zones %>% head()

# I could get the translation (of countries) in English too

# Hey! If the min and max longitude is faily exact it is a very good proxi for the real shapefile (maybe a circle inside the sqare is a little better)

DBI::dbDisconnect(con)
```


This was quite nice, if I want to get the geografies and more data I should look at the dll files (for which I need a compiler from Hex).

## Objective now: 

Objective: 

1. Merge data from all sources with one Nuts3 region as an observation and with

(a) Which UAR have been applied in that zone, (DONE)
(b) from when? (DONE)
(c) any economical data from Eurostat. (DONE)
(d) Look at sources of air pollution (TO DO)
(e) Look at sources of congestion (TO DO)


### Know which zones are treated:

#### Use the coordinates to know which NUTS 3 regions are "treated" OR "not in the control" group

```{r}

# Source: https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts
# (higher quality available)
basis <- "~/A - Estudios/LSE_ASDS/Project/PM2.5/NUTS3/"
e2016 <- "ref-nuts-2016-01m"
e2006 <- "ref-nuts-2006-01m"
wd2016 <- paste(basis, e2016, sep = "")
wd2006 <- paste(basis, e2006, sep = "")


shData2016 <- readOGR(wd2016,
                      "NUTS_RG_01M_2016_4326")
shData2016 <- spTransform(shData2016, 
                          CRS("+proj=longlat +datum=WGS84 +no_defs"))

# class(shData2016)

NUTS1_16 <- shData2016[shData2016$LEVL_CODE == 1, ]
NUTS2_16 <- shData2016[shData2016$LEVL_CODE == 2, ]
NUTS3_16 <- shData2016[shData2016$LEVL_CODE == 3, ]
class(NUTS3_16)

NUTS3_16@data <- NUTS3_16@data %>%
  mutate(NUTS_NAME = NUTS_NAME %>% as.character())

Encoding(NUTS3_16@data$NUTS_NAME) <- "UTF-8"

NUTS3_16@data %>% head()
```


```{r}
# Convert data from cities into geolocations:
## From UAR:

uar <- uar %>% arrange(scheme_color) # to improve the map

# (points)
uar_sp <- SpatialPointsDataFrame(
  coords = cbind(uar$city_longitude, uar$city_latitude),
  data = uar 
)

# uar$points <- SpatialPoints(cbind(uar$city_latitude, uar$city_longitude))
proj4string(uar_sp) <- CRS("+proj=longlat +datum=WGS84 +no_defs")

# NUTS3_16 <- spTransform(NUTS3_16, 
                        # CRS("+proj=longlat +datum=WGS84 +no_defs"))

## From GreenZones:
# (points)
gzones_sp <- SpatialPointsDataFrame(
  coords = cbind(zones$lon, zones$lat),
  data = zones
)

gzones_sp@coords[155,] <- c(18.0280074, 59.3359563)
proj4string(gzones_sp) <- CRS("+proj=longlat +datum=WGS84 +no_defs")

# (polygons)
# 1. Create a list of polygons with the latitudes and longitudes
zones_polygons <- apply(
  X = t(zones[c("MinLongitude", "MinLatitude",
                "MaxLongitude", "MaxLatitude")]),
  MARGIN = 2, 
  function(x){
    Polygon(matrix(c(x[1], x[2], # lower left
                     x[1], x[4], # lower right
                     x[3], x[4], # upper right
                     x[3], x[2], # upper left 
                     x[1], x[2]  # The beguinning again
    ), byrow = TRUE, ncol = 2), hole = FALSE)
  })
# 2. transform them into "spatial polygons" and create a SpatialDF:

# This is going for each polygon, transform it into a "Polygons".
p1 <- lapply(seq_along(zones_polygons), function(i){
  Polygons(list(zones_polygons[[i]]), ID = row.names(zones)[i])
})

sps = SpatialPolygons(p1, 
                      proj4string = CRS("+proj=longlat +datum=WGS84 +no_defs"))

gzones_sp_pol <- SpatialPolygonsDataFrame(sps,
                                          data = zones)

# { # Old static map
#   plot(uar_sp, col = "blue")
#   plot(NUTS3_16, add = TRUE)
#   plot(gzones_sp_pol, col = "red", add = TRUE)
# }

# New Leaflet map
col_map <- c("red", "blue", "green")
col_size <- c(4,3,1,2)

leaflet() %>% 
  addTiles() %>% 
  addPolygons(data = NUTS3_16) %>%
  addPolygons(
    data = gzones_sp_pol,
    popup = paste0("<b> Name: </b>", gzones_sp_pol$KML,  
                   "<br/> <b> In place since: </b>", gzones_sp_pol$InPlaceSince, 
                   "<br/> <b> Country: </b>", gzones_sp_pol$TitelLocal,
                   "<br/> <b> Source: </b> GreenZonesApp")) %>%
  addCircleMarkers(
    data = uar_sp, weight = NA,
    opacity = 0.1,
    color = NA,
    fillColor = col_map[uar_sp$scheme_color],
    fillOpacity = 0.9,
    radius = col_size[uar_sp$scheme_color]*2,
    popup = paste0("<b>", uar_sp@data$cityname, "</b> <br/>",
                   "<b> Scheme: </b>", uar_sp$scheme, "<br/>",
                   "<b> Source: </b> U.A.R")
    )
```


```{r}
# Include if a NUTS region is included in gzones or uar.
gzones_sp_pol %>% head() %>% as.data.frame()
```


Unite city-points/zones with the databases of characteristcs

```{r}
# To unite them I found out a wat to do it with "sf" so I convert them into sf and then go back.

library(sf)

# Convert to sf-objects
nuts.sf <- st_as_sf(NUTS3_16)
uar_points.sf <- st_as_sf(uar_sp)
gzones_sp.sf <- st_as_sf(gzones_sp)
gzones_sp_pol.sf <- st_as_sf(gzones_sp_pol)

# This keeps all the cities (regulations) and includes the LEZ zone in which they "intersect" (default) in and their characterisitics.

uar_points.sf_nuts <- st_join(uar_points.sf, nuts.sf) 
gzones_sp.sf_nuts <- st_join(gzones_sp.sf, nuts.sf) 
gzones_sp_pol.sf_nuts <- st_join(gzones_sp_pol.sf, nuts.sf) 

# We can see the one with poligons has more rows
lapply(list(gzones_sp.sf_nuts, gzones_sp_pol.sf_nuts), dim)

# Look for Paris:
lapply(list(gzones_sp.sf_nuts, gzones_sp_pol.sf_nuts), function(x){
  as.data.frame(x) %>% 
  filter(ElternzoneId == 22)  %>%
  select(NUTS_NAME) %>% as.character()
}) # The zones df includes all NUTS regions in contact with the zone!

# Note: This looks like be easily changed from "intersect" to "covers" or "is_within_distance"


gzones_sp.sf_nuts %>% as.data.frame() %>% 
  filter(ElternzoneId == 22)
?st_join

# st_contains(points.sf, nuts.sf)

plot(uar_points.sf_nuts)
plot(gzones_sp.sf_nuts)
plot(nuts.sf$geometry)

# Convert back to Spatial*
uar_nuts <- as(uar_points.sf_nuts, "Spatial")
gz_nuts <- as(gzones_sp.sf_nuts, "Spatial")
# srdf_meuse <- as(srdf_meuse, "Spatial")


# Create a variable of minimum time for gzones (GZ):
# The time the FIRST measure was applied.
# Note: This is not perfect because it only includes the zone of the point and not evet NUTS included in the LEZ.
min_time <- gzones_sp.sf_nuts %>% as.data.frame() %>%
      dplyr::group_by(NUTS_ID) %>% 
      dplyr::summarise(min_time = min(InPlaceSince))
```

```{r}
min_time
```


Map of LEZ zones affected:
NOTE: Need to add ALL ZONES of a city that are affected. (use the bbox of one data or get the shapefiles of each zone)

```{r}
dta_uar <- uar_points.sf_nuts %>% as.data.frame() %>%
  select(NUTS_ID, NUTS_NAME, scheme)

dta_gz <- gzones_sp.sf_nuts %>% as.data.frame() %>%
  select(NUTS_ID, NUTS_NAME, InPlaceSince)


NUTS3_map <- NUTS3_16
NUTS3_map@data <- NUTS3_16@data %>%
  left_join(dta_uar) %>%
  left_join(dta_gz)

library(tmap)
library(leaflet)
tm(temp, "scheme")
qtm_lez <- tmap::qtm(temp, fill = "scheme", 
                     bbox = c(-10, 30, 50, 71), 
                     borders = NULL)

##  ESTO DA LOS LUGARES EQUIVOCADOS PERO ES UN BUEN INTENTO
# leaflet() %>% addTiles() %>%
#   addPolygons(data = NUTS3_map,
#               popup = paste0("Name: ", NUTS3_map$NUTS_NAME),
#               weight = 0.5,
#               fill = NUTS3_map$scheme)

qtm_lez
tmap::tmap_leaflet(qtm_lez)
```


Include all relevant data in nuts.sf and save it.

```{r}
# Mark NUTS 3 regions that have a city or point marked in eather UAR or GZ
nuts.sf <- nuts.sf %>%
  mutate(
    appl_uar = NUTS_ID %in% uar_points.sf_nuts$NUTS_ID,
    appl_gz = NUTS_ID %in% gzones_sp.sf_nuts$NUTS_ID
    # min_apl_date = gzones_sp.sf_nuts$min_time
  ) %>%
  left_join(min_time) %>%
  select(-c("FID")) # duplicated

nuts.sf %>% as.data.frame() %>% head(20)

st_write(obj = nuts.sf, "data/nuts_sf.shp", delete_layer = TRUE)

# plot(nuts.sf %>% select(appl_uar))
```

